{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dqn_sb3.ipynb","provenance":[],"collapsed_sections":["xVm9QPNVwKXN"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cgDiMlHXXN7V"},"source":["# DQN and Double DQN with Stable-Baselines3\n","\n","Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n","\n","Documentation: https://stable-baselines.readthedocs.io/en/master/\n","\n","RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n","\n","Double Q-Learning: https://paperswithcode.com/method/double-q-learning\n","\n","\n","[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n","\n","It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n","\n","\n","## Introduction\n","\n","In this notebook, we will study DQN using Stable-Baselines3 and then see how to reduce value overestimation with double DQN."]},{"cell_type":"markdown","metadata":{"id":"StmMaKjrX6MC"},"source":["## Installation\n","\n","We will install master version of SB3."]},{"cell_type":"code","metadata":{"id":"r7yLmacAXJ0F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644806680598,"user_tz":300,"elapsed":33804,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}},"outputId":"9d32cd2b-574b-4bf1-96d2-15c1d981e4bb"},"source":["!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n","\n","!pip install git+https://github.com/DLR-RM/stable-baselines3#egg=stable-baselines3[extra]"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","freeglut3-dev is already the newest version (2.8.1-3).\n","freeglut3-dev set to manually installed.\n","ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n","The following packages were automatically installed and are no longer required:\n","  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n","  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n","  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n","  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n","  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n","  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n","  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n","  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n","  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n","  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n","  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n","  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n","  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n","  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n","  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n","  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n","  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n","  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n","  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n","  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n","  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n","  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n","  cuda-visual-tools-11-1 default-jre dkms keyboard-configuration libargon2-0\n","  libcap2 libcryptsetup12 libdevmapper1.02.1 libidn11 libip4tc0 libjansson4\n","  libnvidia-cfg1-510 libnvidia-common-460 libnvidia-common-510\n","  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n","  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxi-dev\n","  libxmu-dev libxmu-headers libxnvctrl0 libxtst6 nsight-compute-2020.2.1\n","  nsight-compute-2022.1.0 nsight-systems-2020.3.2 nsight-systems-2020.3.4\n","  nsight-systems-2021.5.2 nvidia-dkms-510 nvidia-kernel-common-510\n","  nvidia-kernel-source-510 nvidia-modprobe nvidia-settings openjdk-11-jre\n","  policykit-1 policykit-1-gnome python3-xkit screen-resolution-extra systemd\n","  systemd-sysv udev xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n","Use 'apt autoremove' to remove them.\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n","Need to get 784 kB of archives.\n","After this operation, 2,271 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n","Fetched 784 kB in 1s (663 kB/s)\n","Selecting previously unselected package xvfb.\n","(Reading database ... 155113 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting stable-baselines3[extra]\n","  Cloning https://github.com/DLR-RM/stable-baselines3 to /tmp/pip-install-13cxql1q/stable-baselines3_b5a80f325f1d43b78e3b926c5e371f4f\n","  Running command git clone -q https://github.com/DLR-RM/stable-baselines3 /tmp/pip-install-13cxql1q/stable-baselines3_b5a80f325f1d43b78e3b926c5e371f4f\n","Collecting gym>=0.21\n","  Downloading gym-0.21.0.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.19.5)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.10.0+cu111)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.7.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n","Requirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21->stable-baselines3[extra]) (4.10.1)\n","Collecting ale-py~=0.7.1\n","  Downloading ale_py-0.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 34.7 MB/s \n","\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.1->gym>=0.21->stable-baselines3[extra]) (5.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (2.23.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (4.62.3)\n","Collecting AutoROM.accept-rom-license\n","  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym>=0.21->stable-baselines3[extra]) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym>=0.21->stable-baselines3[extra]) (3.10.0.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.43.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.17.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->stable-baselines3[extra]) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n","Building wheels for collected packages: stable-baselines3, gym, AutoROM.accept-rom-license\n","  Building wheel for stable-baselines3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stable-baselines3: filename=stable_baselines3-1.4.1a0-py3-none-any.whl size=165428 sha256=840c0a19a8feae5b89358887c07594a53339442ce9db9e9b332bf30529389a18\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-06nl2jk_/wheels/2b/88/65/5d0cb266b061107af8c518096240bea8578e9843716f79e4da\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616823 sha256=c42bbfdd718f83a059879a3be88fa1b8d97bed4605ee6ea715f6f74d483ceede\n","  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n","  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=7aa6b7ae9cd4bbaf49164355fa9a443c93893cf7d925414b13895f55d7a4cf17\n","  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n","Successfully built stable-baselines3 gym AutoROM.accept-rom-license\n","Installing collected packages: AutoROM.accept-rom-license, autorom, gym, ale-py, stable-baselines3\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.17.3\n","    Uninstalling gym-0.17.3:\n","      Successfully uninstalled gym-0.17.3\n","Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.3 autorom-0.4.2 gym-0.21.0 stable-baselines3-1.4.1a0\n"]}]},{"cell_type":"markdown","metadata":{"id":"LwC8l-17YseR"},"source":["Import DQN and evaluation helper"]},{"cell_type":"code","metadata":{"id":"VYbeqK0tYenp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644806687309,"user_tz":300,"elapsed":6715,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}},"outputId":"973009a6-9197-46b9-de5f-1a917578f9c2"},"source":["import gym\n","import numpy as np\n","import torch as th\n","import matplotlib.pyplot as plt\n","\n","from stable_baselines3 import DQN\n","from stable_baselines3.common.evaluation import evaluate_policy"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n","  for external in metadata.entry_points().get(self.group, []):\n"]}]},{"cell_type":"markdown","metadata":{"id":"7xQwxJDgptxH"},"source":["## The Mountain Car Problem\n","\n","In this environment, the agent must drive an underpowered car up a steep mountain road. For any given state the agent may choose to accelerate to the left, right or cease any acceleration.\n","\n","Source: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n","\n","![mountaincar.png](data:image/png;base64,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)\n","\n","\n","    Observation:\n","        Type: Box(2)\n","        Num    Observation               Min            Max\n","        0      Car Position              -1.2           0.6\n","        1      Car Velocity              -0.07          0.07\n","    Actions:\n","        Type: Discrete(3)\n","        Num    Action\n","        0      Accelerate to the Left\n","        1      Don't accelerate\n","        2      Accelerate to the Right\n","        Note: This does not affect the amount of velocity affected by the\n","        gravitational pull acting on the car.\n","    Reward:\n","         Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n","         on top of the mountain.\n","         Reward of -1 is awarded if the position of the agent is less than 0.5.\n","    Starting State:\n","         The position of the car is assigned a uniform random value in\n","         [-0.6 , -0.4].\n","         The starting velocity of the car is always assigned to 0.\n","    Episode Termination:\n","         The car position is more than 0.5\n","         Episode length is greater than 200\n","\n"]},{"cell_type":"markdown","metadata":{"id":"og59Z62aZJna"},"source":["Create the environment"]},{"cell_type":"code","metadata":{"id":"tVY05GIhZEMM","executionInfo":{"status":"ok","timestamp":1644806687310,"user_tz":300,"elapsed":2,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["env = gym.make(\"MountainCar-v0\")"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IEF4P0DAZMAN"},"source":["Create the model with tuned hyperparameters from the RL Zoo\n","\n","```yaml\n","MountainCar-v0:\n","  n_timesteps: !!float 1.2e5\n","  policy: 'MlpPolicy'\n","  learning_rate: !!float 4e-3\n","  batch_size: 128\n","  buffer_size: 10000\n","  learning_starts: 1000\n","  gamma: 0.98\n","  target_update_interval: 600\n","  train_freq: 16\n","  gradient_steps: 8\n","  exploration_fraction: 0.2\n","  exploration_final_eps: 0.07\n","  policy_kwargs: \"dict(net_arch=[256, 256])\"\n","```"]},{"cell_type":"code","metadata":{"id":"fbEcqWhqgDmH","executionInfo":{"status":"ok","timestamp":1644806693027,"user_tz":300,"elapsed":153,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["tensorboard_log = \"data/tb/\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-1scts3Y1c7","outputId":"3bfe06a4-08a5-4f37-efa1-409dcc8aca62","executionInfo":{"status":"ok","timestamp":1644806693847,"user_tz":300,"elapsed":184,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["dqn_model = DQN(\"MlpPolicy\",\n","            env,\n","            verbose=1,\n","            train_freq=16,\n","            gradient_steps=8,\n","            gamma=0.99,\n","            exploration_fraction=0.2,\n","            exploration_final_eps=0.07,\n","            target_update_interval=600,\n","            learning_starts=1000,\n","            buffer_size=10000,\n","            batch_size=128,\n","            learning_rate=4e-3,\n","            policy_kwargs=dict(net_arch=[256, 256]),\n","            tensorboard_log=tensorboard_log,\n","            seed=2)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n"]}]},{"cell_type":"markdown","metadata":{"id":"eNoFwsCPZQuz"},"source":["Evaluate the agent before training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qn0C7RHyZTHA","outputId":"110c2e56-7456-4bac-ad82-6dcab1fd0d86","executionInfo":{"status":"ok","timestamp":1644806705572,"user_tz":300,"elapsed":1966,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["mean_reward, std_reward = evaluate_policy(dqn_model, dqn_model.get_env(), deterministic=True, n_eval_episodes=20)\n","\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["mean_reward:-200.00 +/- 0.00\n"]}]},{"cell_type":"code","metadata":{"id":"kM0m1NyAgVhR"},"source":["# Optional: Monitor training in tensorboard\n","# %load_ext tensorboard\n","# %tensorboard --logdir $tensorboard_log"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dUoPUfQ4ZexC"},"source":["We will first train the agent until convergence and then analyse the learned q-value function."]},{"cell_type":"code","metadata":{"id":"9wKP0gKjZgWZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c28b45f-f7f6-44f9-9157-2fa5c4c845ca"},"source":["dqn_model.learn(int(1.2e5), log_interval=10)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to data/tb/DQN_1\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.922    |\n","| time/               |          |\n","|    episodes         | 10       |\n","|    fps              | 418      |\n","|    time_elapsed     | 4        |\n","|    total_timesteps  | 2000     |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 3.35e-05 |\n","|    n_updates        | 496      |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.845    |\n","| time/               |          |\n","|    episodes         | 20       |\n","|    fps              | 437      |\n","|    time_elapsed     | 9        |\n","|    total_timesteps  | 4000     |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 1.39e-06 |\n","|    n_updates        | 1496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.767    |\n","| time/               |          |\n","|    episodes         | 30       |\n","|    fps              | 440      |\n","|    time_elapsed     | 13       |\n","|    total_timesteps  | 6000     |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 5.11e-07 |\n","|    n_updates        | 2496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.69     |\n","| time/               |          |\n","|    episodes         | 40       |\n","|    fps              | 439      |\n","|    time_elapsed     | 18       |\n","|    total_timesteps  | 8000     |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 1.91e-05 |\n","|    n_updates        | 3496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.612    |\n","| time/               |          |\n","|    episodes         | 50       |\n","|    fps              | 436      |\n","|    time_elapsed     | 22       |\n","|    total_timesteps  | 10000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 1.6e-06  |\n","|    n_updates        | 4496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.535    |\n","| time/               |          |\n","|    episodes         | 60       |\n","|    fps              | 434      |\n","|    time_elapsed     | 27       |\n","|    total_timesteps  | 12000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 1.38e-06 |\n","|    n_updates        | 5496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.457    |\n","| time/               |          |\n","|    episodes         | 70       |\n","|    fps              | 431      |\n","|    time_elapsed     | 32       |\n","|    total_timesteps  | 14000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 1.25e-05 |\n","|    n_updates        | 6496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.38     |\n","| time/               |          |\n","|    episodes         | 80       |\n","|    fps              | 426      |\n","|    time_elapsed     | 37       |\n","|    total_timesteps  | 16000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 4.47e-06 |\n","|    n_updates        | 7496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.302    |\n","| time/               |          |\n","|    episodes         | 90       |\n","|    fps              | 421      |\n","|    time_elapsed     | 42       |\n","|    total_timesteps  | 18000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 2.22e-06 |\n","|    n_updates        | 8496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.225    |\n","| time/               |          |\n","|    episodes         | 100      |\n","|    fps              | 419      |\n","|    time_elapsed     | 47       |\n","|    total_timesteps  | 20000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 2.15e-05 |\n","|    n_updates        | 9496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.147    |\n","| time/               |          |\n","|    episodes         | 110      |\n","|    fps              | 417      |\n","|    time_elapsed     | 52       |\n","|    total_timesteps  | 22000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 5.86e-06 |\n","|    n_updates        | 10496    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.0709   |\n","| time/               |          |\n","|    episodes         | 120      |\n","|    fps              | 414      |\n","|    time_elapsed     | 57       |\n","|    total_timesteps  | 23978    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 1.28e-05 |\n","|    n_updates        | 11488    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 199      |\n","|    ep_rew_mean      | -199     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 130      |\n","|    fps              | 411      |\n","|    time_elapsed     | 63       |\n","|    total_timesteps  | 25944    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.000472 |\n","|    n_updates        | 12472    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 199      |\n","|    ep_rew_mean      | -199     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 140      |\n","|    fps              | 402      |\n","|    time_elapsed     | 69       |\n","|    total_timesteps  | 27944    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 4.07e-05 |\n","|    n_updates        | 13472    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 199      |\n","|    ep_rew_mean      | -199     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 150      |\n","|    fps              | 400      |\n","|    time_elapsed     | 74       |\n","|    total_timesteps  | 29944    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.00012  |\n","|    n_updates        | 14472    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 198      |\n","|    ep_rew_mean      | -198     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 160      |\n","|    fps              | 398      |\n","|    time_elapsed     | 79       |\n","|    total_timesteps  | 31839    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.058    |\n","|    n_updates        | 15416    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 198      |\n","|    ep_rew_mean      | -198     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 170      |\n","|    fps              | 397      |\n","|    time_elapsed     | 85       |\n","|    total_timesteps  | 33804    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.00101  |\n","|    n_updates        | 16400    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 198      |\n","|    ep_rew_mean      | -198     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 180      |\n","|    fps              | 396      |\n","|    time_elapsed     | 90       |\n","|    total_timesteps  | 35762    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.024    |\n","|    n_updates        | 17384    |\n","----------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"tI8xh463Zi7M"},"source":["Evaluate after training, the mean episodic reward should have improved."]},{"cell_type":"code","metadata":{"id":"z952YogYZ8yD"},"source":["mean_reward, std_reward = evaluate_policy(dqn_model, dqn_model.get_env(), deterministic=True, n_eval_episodes=20)\n","\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVm9QPNVwKXN"},"source":["### Prepare video recording"]},{"cell_type":"code","metadata":{"id":"MPyfQxD5z26J"},"source":["# Set up fake display; otherwise rendering will fail\n","import os\n","os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n","os.environ['DISPLAY'] = ':1'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLzXxO8VMD6N"},"source":["import base64\n","from pathlib import Path\n","\n","from IPython import display as ipythondisplay\n","\n","def show_videos(video_path='', prefix=''):\n","  \"\"\"\n","  Taken from https://github.com/eleurent/highway-env\n","\n","  :param video_path: (str) Path to the folder containing videos\n","  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n","  \"\"\"\n","  html = []\n","  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n","      video_b64 = base64.b64encode(mp4.read_bytes())\n","      html.append('''<video alt=\"{}\" autoplay \n","                    loop controls style=\"height: 400px;\">\n","                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n","                </video>'''.format(mp4, video_b64.decode('ascii')))\n","  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LTRNUfulOGaF"},"source":["We will record a video using the [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."]},{"cell_type":"code","metadata":{"id":"Trag9dQpOIhx"},"source":["from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n","\n","def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n","  \"\"\"\n","  :param env_id: (str)\n","  :param model: (RL model)\n","  :param video_length: (int)\n","  :param prefix: (str)\n","  :param video_folder: (str)\n","  \"\"\"\n","  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n","  # Start the video at step=0 and record 500 steps\n","  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n","                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n","                              name_prefix=prefix)\n","\n","  obs = eval_env.reset()\n","  for _ in range(video_length):\n","    action, _ = model.predict(obs, deterministic=False)\n","    obs, _, _, _ = eval_env.step(action)\n","\n","  # Close the video recorder\n","  eval_env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFiOqKE3aDzI"},"source":["## Visualize trained agent"]},{"cell_type":"code","metadata":{"id":"MvVGX13xaGbf"},"source":["record_video('MountainCar-v0', dqn_model, video_length=500, prefix='dqn-mountaincar')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHaYvk8uaK70"},"source":["show_videos('videos', prefix='dqn')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TY00dtf4aRLQ"},"source":["## Visualize Q-values"]},{"cell_type":"markdown","metadata":{"id":"cP3YH2x7rieM"},"source":["### Exercise (5 minutes): Retrieve q-values\n","\n","The function will be used to retrieve the learned q-values for a given state (`observation` in the code).\n","\n","The q-network from SB3 DQN can be accessed via `model.q_net` and is a PyTorch module (you can therefore call `.forward()` on it).\n","\n","You need to convert the observation to a PyTorch tensor and then convert the resulting q-values to numpy array.\n","\n","Note: It is recommended to use `with th.no_grad():` context to save computation and memory"]},{"cell_type":"code","metadata":{"id":"QiCp8OpCbKZW"},"source":["def get_q_values(model: DQN, obs: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Retrieve Q-values for a given observation.\n","\n","    :param model: a DQN model\n","    :param obs: a single observation\n","    :return: the associated q-values for the given observation\n","    \"\"\"\n","    assert model.get_env().observation_space.contains(obs), f\"Invalid observation of shape {obs.shape}: {obs}\"\n","    ### YOUR CODE HERE\n","    # Retrieve q-values for the given observation and convert them to numpy\n","    \n","    ### END OF YOUR CODE\n","    assert isinstance(q_values, np.ndarray), \"The returned q_values is not a numpy array\"\n","    assert q_values.shape == (3,), f\"Wrong shape: (3,) was expected but got {q_values.shape}\"\n","\n","    return q_values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcudYVkdbB0e"},"source":["### Q-values for the initial state\n","\n","Let's reset the environment to start a new episode:"]},{"cell_type":"code","metadata":{"id":"izFixcWgaVe3"},"source":["obs = env.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ouWnFaut0KB"},"source":["we plot the rendered environment to visualize it"]},{"cell_type":"code","metadata":{"id":"NF1L_1Gfal-g"},"source":["plt.axis('off')\n","plt.imshow(env.render(mode=\"rgb_array\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rtKIGyJ-bEQO"},"source":["### Exercise (5 minutes): predict taken action according to q-values\n","\n","Using the `get_q_values()` function, retrieve the q-values for the initial observation, print them for each action (\"left\", \"nothing\", \"right\") and print the action that the greedy (deterministic) policy would follow (i.e., the action with the highest q-value for that state)."]},{"cell_type":"code","metadata":{"id":"BPegCYbSuY-f"},"source":["action_str = [\"Left\", \"Nothing\", \"Right\"]  # action=0 -> go left, action=1 -> do nothing, action=2 -> go right"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bv90MTHvaqbV"},"source":["### YOUR CODE HERE\n","# Retrieve q-values for the initial state\n","# You should use `get_q_values()`\n","\n","### END OF YOUR CODE\n","\n","print(f\"Q-value of the initial state left={q_value_left:.2f} nothing={q_value_nothing:.2f} right={q_value_right:.2f}\")\n","\n","### YOUR CODE HERE\n","# Compute the action taken in the initilal state according to q-values \n","# when following a greedy strategy\n","\n","\n","## END of your code here\n","\n","print(f\"Action taken by the greedy policy in the initial state: {action_str[action]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ov0UwoBzcaDv"},"source":["The q-value of the initial state corresponds to how much (discounted) reward the agent expects to get in this episode.\n","\n","We will compare the estimated q-value to the discounted return of the episode."]},{"cell_type":"code","metadata":{"id":"dhhF-GJccVne"},"source":["initial_q_value = q_values.max()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JueeSE1xcQpK"},"source":["## Step until the end of the episode\n","\n"]},{"cell_type":"code","metadata":{"id":"V_OYobAab8SF"},"source":["episode_rewards = []\n","done = False\n","i = 0\n","\n","while not done:\n","    i += 1\n","\n","    # Display current state\n","    plt.imshow(env.render(mode=\"rgb_array\"))\n","    plt.show()\n","\n","    # Retrieve q-value\n","    q_values = get_q_values(dqn_model, obs)\n","\n","    # Take greedy-action\n","    action, _ = dqn_model.predict(obs, deterministic=True)\n","\n","    print(f\"Q-value of the current state left={q_values[0]:.2f} nothing={q_values[1]:.2f} right={q_values[2]:.2f}\")\n","    print(f\"Action: {action_str[action]}\")\n","\n","    obs, reward, done, info = env.step(action)\n","\n","    episode_rewards.append(reward)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YBEomET1wkjN"},"source":["### Exercise (3 minutes): compare estimated initial q-value with actual discounted return\n","\n","Compute the discounted return (sum of discounted reward) of the episode and compare it to the initial estimated q-value.\n","\n","Note: You will need to use the discount factor `dqn_model.gamma`"]},{"cell_type":"code","metadata":{"id":"Om4NNW2VdnM9"},"source":["sum_discounted_rewards = 0\n","\n","### YOUR CODE HERE\n","# Compute the sum of discounted reward for the last episode\n","# using `episode_rewards` list and `dqn_model.gamma` discount factor\n","\n","### END OF YOUR CODE\n","\n","print(f\"Sum discounted rewards: {sum_discounted_rewards:.2f}, initial q-value {initial_q_value:.2f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZloxyPKPg9HX"},"source":["## Exercise (30 minutes): Double DQN\n","\n","In DQN, the same network is responsible for selecting and estimating the best next action (in the TD-target) and that may lead to over-estimation (the action which q-value is over-estimated will be chosen more often and this slow down training).\n","\n","To reduce over-estimation, double q-learning (and then double DQN) was proposed. It decouples the action selection from the value estimation.\n","\n","Concretely, in DQN, the target q-value is defined as:\n","\n","$$Y^{DQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right); \\mathbb{\\theta}_{target}\\right)$$\n","\n","where the target network `q_net_target` with parameters $\\mathbb{\\theta}_{target}$ is used for both action selection and estimation, and can therefore be rewritten:\n","\n","$$Y^{DQN}_{t} = r_{t+1} + \\gamma \\max_{a}{Q}\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right)$$\n","\n","Double DQN uses the online network `q_net` with parameters $\\mathbb{\\theta}_{online}$ to select the action and the target network `q_net_target` to estimate the associated q-values:\n","\n","$$Y^{DoubleDQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{online}\\right); \\mathbb{\\theta}_{target}\\right)$$\n","\n","\n","The goal in this exercise is for you to write the update method for `DoubleDQN`.\n","\n","You will need to:\n","\n","1. Sample replay buffer data using `self.replay_buffer.sample(batch_size)`\n","\n","2. Compute the Double DQN target q-value using the next observations `replay_data.next_observation`, the online network `self.q_net`, the target network `self.q_net_target`, the rewards `replay_data.rewards` and the termination signals `replay_data.dones`. Be careful with the shape of each object ;)\n","\n","3. Compute the current q-value estimates using the online network `self.q_net`, the current observations `replay_data.observations` and the buffer actions `replay_data.actions`\n","\n","4. Compute the loss to train the q-network using L2 or Huber loss (`F.smooth_l1_loss`)\n","\n","\n","Link: https://paperswithcode.com/method/double-q-learning\n","\n","Paper: https://arxiv.org/abs/1509.06461\n","\n"]},{"cell_type":"code","metadata":{"id":"4227ILqjg8b4"},"source":["from torch.nn import functional as F\n","\n","class DoubleDQN(DQN):\n","    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n","        # Switch to train mode (this affects batch norm / dropout)\n","        self.policy.set_training_mode(True)\n","        # Update learning rate according to schedule\n","        self._update_learning_rate(self.policy.optimizer)\n","\n","        losses = []\n","        for _ in range(gradient_steps):\n","            ### YOUR CODE HERE\n","            # Sample replay buffer\n","            replay_data = ...\n","\n","            # Do not backpropagate gradient to the target network\n","            with th.no_grad():\n","                # Compute the next Q-values using the target network\n","                next_q_values = ...\n","                # Decouple action selection from value estimation\n","                # Compute q-values for the next observation using the online q net\n","                next_q_values_online = ...\n","                # Select action with online network\n","                next_actions_online = ...\n","                # Estimate the q-values for the selected actions using target q network\n","                next_q_values = ...\n","               \n","                # 1-step TD target\n","                target_q_values = ...\n","\n","            # Get current Q-values estimates\n","            current_q_values = ...\n","\n","            # Retrieve the q-values for the actions from the replay buffer\n","            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n","\n","            # Check the shape\n","            assert current_q_values.shape == target_q_values.shape\n","\n","            # Compute loss (L2 or Huber loss)\n","            loss = ...\n","\n","            ### END OF YOUR CODE\n","            \n","            losses.append(loss.item())\n","\n","            # Optimize the q-network\n","            self.policy.optimizer.zero_grad()\n","            loss.backward()\n","            # Clip gradient norm\n","            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n","            self.policy.optimizer.step()\n","\n","        # Increase update counter\n","        self._n_updates += gradient_steps\n","\n","        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n","        self.logger.record(\"train/loss\", np.mean(losses))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dG3fpWWOg_AS"},"source":["## Monitoring Evolution of the Estimated q-value\n","\n","\n","Here we create a SB3 callback to over-estimate initial q-values and then monitor their evolution over time."]},{"cell_type":"code","metadata":{"id":"BLbQ9RhUpMOl"},"source":["from torch.nn import functional as F\n","\n","from stable_baselines3.common.callbacks import BaseCallback\n","\n","\n","class MonitorQValueCallback(BaseCallback):\n","    \"\"\"\n","    Callback to monitor the evolution of the q-value\n","    for the initial state.\n","    It allows to artificially over-estimate a q-value for initial states.\n","\n","    \"\"\"\n","    def __init__(self, sample_interval: int = 2500):\n","        super().__init__()\n","        self.timesteps = []\n","        self.max_q_values = []\n","        self.sample_interval = sample_interval\n","        n_samples = 512\n","        env = gym.make(\"MountainCar-v0\")\n","        # Sample initial states that will be used to monitor the estimated q-value\n","        self.start_obs = np.array([env.reset() for _ in range(n_samples)])\n","    \n","    def _on_training_start(self) -> None:\n","        # Create overestimation\n","        obs = th.tensor(self.start_obs, device=self.model.device).float()\n","        # Over-estimate going left q-value for the initial states\n","        target_q_values = th.ones((len(obs), 1), device=self.model.device).float() * 100\n","\n","        for _ in range(100):\n","            # Get current Q-values estimates\n","            current_q_values = self.model.q_net(obs)\n","\n","            # Over-estimate going left\n","            current_q_values = th.gather(current_q_values, dim=1, index=th.zeros((len(obs), 1), device=self.model.device).long())\n","\n","            loss = F.mse_loss(current_q_values, target_q_values)\n","\n","            # Optimize the policy\n","            self.model.policy.optimizer.zero_grad()\n","            loss.backward()\n","            self.model.policy.optimizer.step()\n","\n","    def _on_step(self) -> bool:\n","        # Sample q-values\n","        if self.n_calls % self.sample_interval == 0:\n","            # Monitor estimated q-values using current model\n","            obs = th.tensor(self.start_obs, device=self.model.device).float()\n","            with th.no_grad():\n","                q_values = self.model.q_net(obs).cpu().numpy()\n","\n","            self.logger.record(\"train/max_q_value\", float(q_values.max()))\n","            self.timesteps.append(self.num_timesteps)\n","            self.max_q_values.append(q_values.max())\n","        return True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36dtW1c4xQUG"},"source":["## Evolution of the q-value with initial over-estimation"]},{"cell_type":"markdown","metadata":{"id":"j-XTmT6SxdOa"},"source":["### DQN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZIo1EDx2xcwZ","outputId":"aac8b11a-e4d6-4278-945e-f48070289f1d"},"source":["dqn_model = DQN(\"MlpPolicy\",\n","            \"MountainCar-v0\",\n","            verbose=1,\n","            train_freq=16,\n","            gradient_steps=8,\n","            gamma=0.99,\n","            exploration_fraction=0.2,\n","            exploration_final_eps=0.07,\n","            target_update_interval=5000,\n","            learning_starts=1000,\n","            buffer_size=25000,\n","            batch_size=128,\n","            learning_rate=4e-3,\n","            policy_kwargs=dict(net_arch=[256, 256]),\n","            tensorboard_log=tensorboard_log,\n","            seed=102)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Creating environment from the given name 'MountainCar-v0'\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n"]}]},{"cell_type":"markdown","metadata":{"id":"Vd62HoWsxfBJ"},"source":["Define the callback"]},{"cell_type":"code","metadata":{"id":"wcULdR48xhH5"},"source":["monitor_dqn_value_cb = MonitorQValueCallback()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Z_cFIapxhR7"},"source":["dqn_model.learn(total_timesteps=int(4e4), callback=monitor_dqn_value_cb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NxAGOezBx3GS"},"source":["### Double DQN"]},{"cell_type":"code","metadata":{"id":"xYNcgKsSegj0"},"source":["double_q = DoubleDQN(\"MlpPolicy\",\n","            \"MountainCar-v0\",\n","            verbose=1,\n","            train_freq=16,\n","            gradient_steps=8,\n","            gamma=0.99,\n","            exploration_fraction=0.2,\n","            exploration_final_eps=0.07,\n","            target_update_interval=5000,\n","            learning_starts=1000,\n","            buffer_size=25000,\n","            batch_size=128,\n","            learning_rate=4e-3,\n","            policy_kwargs=dict(net_arch=[256, 256]),\n","            tensorboard_log=tensorboard_log,\n","            seed=102)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JWhQTCXQyBJZ"},"source":["monitor_double_q_value_cb = MonitorQValueCallback()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mvveywQUkEpW"},"source":["double_q.learn(int(4e4), log_interval=10, callback=monitor_double_q_value_cb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rPqcCnbnyIR5"},"source":["### Evolution of the max q-value for start states over time"]},{"cell_type":"code","metadata":{"id":"jNeKzUoaa6_K"},"source":["plt.figure(figsize=(6, 3), dpi=200)\n","plt.title(\"Evolution of max q-value for start states over time\")\n","plt.plot(monitor_dqn_value_cb.timesteps, monitor_dqn_value_cb.max_q_values, label=\"DQN\")\n","plt.plot(monitor_double_q_value_cb.timesteps, monitor_double_q_value_cb.max_q_values, label=\"Double DQN\")\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqRixViAv6gT"},"source":[""],"execution_count":null,"outputs":[]}]}