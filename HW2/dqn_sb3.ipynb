{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dqn_sb3.ipynb","provenance":[],"collapsed_sections":["xVm9QPNVwKXN"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"cgDiMlHXXN7V"},"source":["# DQN and Double DQN with Stable-Baselines3\n","\n","Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n","\n","Documentation: https://stable-baselines.readthedocs.io/en/master/\n","\n","RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n","\n","Double Q-Learning: https://paperswithcode.com/method/double-q-learning\n","\n","\n","[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n","\n","It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n","\n","\n","## Introduction\n","\n","In this notebook, we will study DQN using Stable-Baselines3 and then see how to reduce value overestimation with double DQN."]},{"cell_type":"markdown","metadata":{"id":"StmMaKjrX6MC"},"source":["## Installation\n","\n","We will install master version of SB3."]},{"cell_type":"code","metadata":{"id":"r7yLmacAXJ0F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644810575372,"user_tz":300,"elapsed":6193,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}},"outputId":"53288759-43cc-4054-c481-b6794aed8681"},"source":["!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n","\n","!pip install git+https://github.com/DLR-RM/stable-baselines3#egg=stable-baselines3[extra]"],"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","freeglut3-dev is already the newest version (2.8.1-3).\n","ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n","The following packages were automatically installed and are no longer required:\n","  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n","  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n","  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n","  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n","  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n","  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n","  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n","  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n","  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n","  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n","  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n","  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n","  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n","  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n","  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n","  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n","  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n","  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n","  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n","  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n","  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n","  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n","  cuda-visual-tools-11-1 default-jre dkms keyboard-configuration libargon2-0\n","  libcap2 libcryptsetup12 libdevmapper1.02.1 libidn11 libip4tc0 libjansson4\n","  libnvidia-cfg1-510 libnvidia-common-460 libnvidia-common-510\n","  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n","  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxi-dev\n","  libxmu-dev libxmu-headers libxnvctrl0 libxtst6 nsight-compute-2020.2.1\n","  nsight-compute-2022.1.0 nsight-systems-2020.3.2 nsight-systems-2020.3.4\n","  nsight-systems-2021.5.2 nvidia-dkms-510 nvidia-kernel-common-510\n","  nvidia-kernel-source-510 nvidia-modprobe nvidia-settings openjdk-11-jre\n","  policykit-1 policykit-1-gnome python3-xkit screen-resolution-extra systemd\n","  systemd-sysv udev xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n","Use 'apt autoremove' to remove them.\n","0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n","Collecting stable-baselines3[extra]\n","  Cloning https://github.com/DLR-RM/stable-baselines3 to /tmp/pip-install-sw497mbt/stable-baselines3_7d190f974e2c46509b5212c9235e514b\n","  Running command git clone -q https://github.com/DLR-RM/stable-baselines3 /tmp/pip-install-sw497mbt/stable-baselines3_7d190f974e2c46509b5212c9235e514b\n","Requirement already satisfied: gym>=0.21 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.21.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.19.5)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.10.0+cu111)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.7.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n","Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21->stable-baselines3[extra]) (4.10.1)\n","Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21->stable-baselines3[extra]) (0.4.2)\n","Requirement already satisfied: ale-py~=0.7.1 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21->stable-baselines3[extra]) (0.7.3)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.1->gym>=0.21->stable-baselines3[extra]) (5.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (2.23.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (4.62.3)\n","Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (0.4.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.21->stable-baselines3[extra]) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.21->stable-baselines3[extra]) (3.7.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.6)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.17.3)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.43.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->stable-baselines3[extra]) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym>=0.21->stable-baselines3[extra]) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n"]}]},{"cell_type":"markdown","metadata":{"id":"LwC8l-17YseR"},"source":["Import DQN and evaluation helper"]},{"cell_type":"code","metadata":{"id":"VYbeqK0tYenp","executionInfo":{"status":"ok","timestamp":1644810566181,"user_tz":300,"elapsed":288,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["import gym\n","import numpy as np\n","import torch as th\n","import matplotlib.pyplot as plt\n","\n","from stable_baselines3 import DQN\n","from stable_baselines3.common.evaluation import evaluate_policy"],"execution_count":65,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7xQwxJDgptxH"},"source":["## The Mountain Car Problem\n","\n","In this environment, the agent must drive an underpowered car up a steep mountain road. For any given state the agent may choose to accelerate to the left, right or cease any acceleration.\n","\n","Source: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n","\n","![mountaincar.png](data:image/png;base64,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)\n","\n","\n","    Observation:\n","        Type: Box(2)\n","        Num    Observation               Min            Max\n","        0      Car Position              -1.2           0.6\n","        1      Car Velocity              -0.07          0.07\n","    Actions:\n","        Type: Discrete(3)\n","        Num    Action\n","        0      Accelerate to the Left\n","        1      Don't accelerate\n","        2      Accelerate to the Right\n","        Note: This does not affect the amount of velocity affected by the\n","        gravitational pull acting on the car.\n","    Reward:\n","         Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n","         on top of the mountain.\n","         Reward of -1 is awarded if the position of the agent is less than 0.5.\n","    Starting State:\n","         The position of the car is assigned a uniform random value in\n","         [-0.6 , -0.4].\n","         The starting velocity of the car is always assigned to 0.\n","    Episode Termination:\n","         The car position is more than 0.5\n","         Episode length is greater than 200\n","\n"]},{"cell_type":"markdown","metadata":{"id":"og59Z62aZJna"},"source":["Create the environment"]},{"cell_type":"code","metadata":{"id":"tVY05GIhZEMM","executionInfo":{"status":"ok","timestamp":1644808028073,"user_tz":300,"elapsed":341,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["env = gym.make(\"MountainCar-v0\")"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IEF4P0DAZMAN"},"source":["Create the model with tuned hyperparameters from the RL Zoo\n","\n","```yaml\n","MountainCar-v0:\n","  n_timesteps: !!float 1.2e5\n","  policy: 'MlpPolicy'\n","  learning_rate: !!float 4e-3\n","  batch_size: 128\n","  buffer_size: 10000\n","  learning_starts: 1000\n","  gamma: 0.98\n","  target_update_interval: 600\n","  train_freq: 16\n","  gradient_steps: 8\n","  exploration_fraction: 0.2\n","  exploration_final_eps: 0.07\n","  policy_kwargs: \"dict(net_arch=[256, 256])\"\n","```"]},{"cell_type":"code","metadata":{"id":"fbEcqWhqgDmH","executionInfo":{"status":"ok","timestamp":1644808031567,"user_tz":300,"elapsed":294,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["tensorboard_log = \"data/tb/\""],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-1scts3Y1c7","outputId":"e2437adf-2ace-43d5-fc9d-3cecf3572620","executionInfo":{"status":"ok","timestamp":1644808036148,"user_tz":300,"elapsed":3062,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["dqn_model = DQN(\"MlpPolicy\",\n","            env,\n","            verbose=1,\n","            train_freq=16,\n","            gradient_steps=8,\n","            gamma=0.99,\n","            exploration_fraction=0.2,\n","            exploration_final_eps=0.07,\n","            target_update_interval=600,\n","            learning_starts=1000,\n","            buffer_size=10000,\n","            batch_size=128,\n","            learning_rate=4e-3,\n","            policy_kwargs=dict(net_arch=[256, 256]),\n","            tensorboard_log=tensorboard_log,\n","            seed=2)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n"]}]},{"cell_type":"markdown","metadata":{"id":"eNoFwsCPZQuz"},"source":["Evaluate the agent before training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qn0C7RHyZTHA","outputId":"8223aa5f-26fb-430e-b076-3b1c48400188","executionInfo":{"status":"ok","timestamp":1644808041392,"user_tz":300,"elapsed":2482,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["mean_reward, std_reward = evaluate_policy(dqn_model, dqn_model.get_env(), deterministic=True, n_eval_episodes=20)\n","\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["mean_reward:-200.00 +/- 0.00\n"]}]},{"cell_type":"code","metadata":{"id":"kM0m1NyAgVhR","executionInfo":{"status":"ok","timestamp":1644807740607,"user_tz":300,"elapsed":6,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["# Optional: Monitor training in tensorboard\n","# %load_ext tensorboard\n","# %tensorboard --logdir $tensorboard_log"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dUoPUfQ4ZexC"},"source":["We will first train the agent until convergence and then analyse the learned q-value function."]},{"cell_type":"code","metadata":{"id":"9wKP0gKjZgWZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644808276060,"user_tz":300,"elapsed":205429,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}},"outputId":"12430229-d3b7-4f33-dfe9-fc65b4cf9125"},"source":["%%time\n","dqn_model.learn(int(1.2e5), log_interval=10)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to data/tb/DQN_4\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.922    |\n","| time/               |          |\n","|    episodes         | 10       |\n","|    fps              | 993      |\n","|    time_elapsed     | 2        |\n","|    total_timesteps  | 2000     |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 2.89e-05 |\n","|    n_updates        | 496      |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.845    |\n","| time/               |          |\n","|    episodes         | 20       |\n","|    fps              | 845      |\n","|    time_elapsed     | 4        |\n","|    total_timesteps  | 4000     |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 6.18e-07 |\n","|    n_updates        | 1496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.767    |\n","| time/               |          |\n","|    episodes         | 30       |\n","|    fps              | 792      |\n","|    time_elapsed     | 7        |\n","|    total_timesteps  | 6000     |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 1.3e-06  |\n","|    n_updates        | 2496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.69     |\n","| time/               |          |\n","|    episodes         | 40       |\n","|    fps              | 763      |\n","|    time_elapsed     | 10       |\n","|    total_timesteps  | 8000     |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 3.29e-05 |\n","|    n_updates        | 3496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.612    |\n","| time/               |          |\n","|    episodes         | 50       |\n","|    fps              | 746      |\n","|    time_elapsed     | 13       |\n","|    total_timesteps  | 10000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 5.36e-06 |\n","|    n_updates        | 4496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.535    |\n","| time/               |          |\n","|    episodes         | 60       |\n","|    fps              | 731      |\n","|    time_elapsed     | 16       |\n","|    total_timesteps  | 12000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 7.37e-06 |\n","|    n_updates        | 5496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.457    |\n","| time/               |          |\n","|    episodes         | 70       |\n","|    fps              | 717      |\n","|    time_elapsed     | 19       |\n","|    total_timesteps  | 14000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 2.46e-05 |\n","|    n_updates        | 6496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.38     |\n","| time/               |          |\n","|    episodes         | 80       |\n","|    fps              | 701      |\n","|    time_elapsed     | 22       |\n","|    total_timesteps  | 16000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 1.53e-05 |\n","|    n_updates        | 7496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.302    |\n","| time/               |          |\n","|    episodes         | 90       |\n","|    fps              | 689      |\n","|    time_elapsed     | 26       |\n","|    total_timesteps  | 18000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 1.23e-05 |\n","|    n_updates        | 8496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.225    |\n","| time/               |          |\n","|    episodes         | 100      |\n","|    fps              | 679      |\n","|    time_elapsed     | 29       |\n","|    total_timesteps  | 20000    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 3.37e-05 |\n","|    n_updates        | 9496     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.149    |\n","| time/               |          |\n","|    episodes         | 110      |\n","|    fps              | 671      |\n","|    time_elapsed     | 32       |\n","|    total_timesteps  | 21954    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0272   |\n","|    n_updates        | 10480    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 200      |\n","|    ep_rew_mean      | -200     |\n","|    exploration_rate | 0.0718   |\n","| time/               |          |\n","|    episodes         | 120      |\n","|    fps              | 663      |\n","|    time_elapsed     | 36       |\n","|    total_timesteps  | 23954    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 4.95e-05 |\n","|    n_updates        | 11480    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 198      |\n","|    ep_rew_mean      | -198     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 130      |\n","|    fps              | 656      |\n","|    time_elapsed     | 39       |\n","|    total_timesteps  | 25832    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0905   |\n","|    n_updates        | 12416    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 198      |\n","|    ep_rew_mean      | -198     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 140      |\n","|    fps              | 650      |\n","|    time_elapsed     | 42       |\n","|    total_timesteps  | 27806    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.000167 |\n","|    n_updates        | 13400    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 197      |\n","|    ep_rew_mean      | -197     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 150      |\n","|    fps              | 644      |\n","|    time_elapsed     | 46       |\n","|    total_timesteps  | 29717    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.00142  |\n","|    n_updates        | 14360    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 196      |\n","|    ep_rew_mean      | -196     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 160      |\n","|    fps              | 640      |\n","|    time_elapsed     | 49       |\n","|    total_timesteps  | 31632    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0219   |\n","|    n_updates        | 15312    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 196      |\n","|    ep_rew_mean      | -196     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 170      |\n","|    fps              | 635      |\n","|    time_elapsed     | 52       |\n","|    total_timesteps  | 33598    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0234   |\n","|    n_updates        | 16296    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 195      |\n","|    ep_rew_mean      | -195     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 180      |\n","|    fps              | 631      |\n","|    time_elapsed     | 56       |\n","|    total_timesteps  | 35456    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0451   |\n","|    n_updates        | 17224    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 194      |\n","|    ep_rew_mean      | -194     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 190      |\n","|    fps              | 619      |\n","|    time_elapsed     | 60       |\n","|    total_timesteps  | 37389    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0152   |\n","|    n_updates        | 18192    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 191      |\n","|    ep_rew_mean      | -191     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 200      |\n","|    fps              | 617      |\n","|    time_elapsed     | 63       |\n","|    total_timesteps  | 39114    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.00765  |\n","|    n_updates        | 19056    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 188      |\n","|    ep_rew_mean      | -188     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 210      |\n","|    fps              | 615      |\n","|    time_elapsed     | 66       |\n","|    total_timesteps  | 40738    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.00383  |\n","|    n_updates        | 19872    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 185      |\n","|    ep_rew_mean      | -185     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 220      |\n","|    fps              | 613      |\n","|    time_elapsed     | 69       |\n","|    total_timesteps  | 42473    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.00684  |\n","|    n_updates        | 20736    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 184      |\n","|    ep_rew_mean      | -184     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 230      |\n","|    fps              | 611      |\n","|    time_elapsed     | 72       |\n","|    total_timesteps  | 44243    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0161   |\n","|    n_updates        | 21624    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 181      |\n","|    ep_rew_mean      | -181     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 240      |\n","|    fps              | 610      |\n","|    time_elapsed     | 75       |\n","|    total_timesteps  | 45881    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0151   |\n","|    n_updates        | 22440    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 178      |\n","|    ep_rew_mean      | -178     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 250      |\n","|    fps              | 609      |\n","|    time_elapsed     | 78       |\n","|    total_timesteps  | 47557    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0282   |\n","|    n_updates        | 23280    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 175      |\n","|    ep_rew_mean      | -175     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 260      |\n","|    fps              | 608      |\n","|    time_elapsed     | 80       |\n","|    total_timesteps  | 49098    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0266   |\n","|    n_updates        | 24048    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 173      |\n","|    ep_rew_mean      | -173     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 270      |\n","|    fps              | 607      |\n","|    time_elapsed     | 83       |\n","|    total_timesteps  | 50864    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0306   |\n","|    n_updates        | 24928    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 172      |\n","|    ep_rew_mean      | -172     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 280      |\n","|    fps              | 606      |\n","|    time_elapsed     | 86       |\n","|    total_timesteps  | 52661    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0488   |\n","|    n_updates        | 25832    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 169      |\n","|    ep_rew_mean      | -169     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 290      |\n","|    fps              | 605      |\n","|    time_elapsed     | 89       |\n","|    total_timesteps  | 54319    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0381   |\n","|    n_updates        | 26656    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 169      |\n","|    ep_rew_mean      | -169     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 300      |\n","|    fps              | 605      |\n","|    time_elapsed     | 92       |\n","|    total_timesteps  | 55970    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0351   |\n","|    n_updates        | 27488    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 170      |\n","|    ep_rew_mean      | -170     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 310      |\n","|    fps              | 604      |\n","|    time_elapsed     | 95       |\n","|    total_timesteps  | 57695    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0322   |\n","|    n_updates        | 28344    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 169      |\n","|    ep_rew_mean      | -169     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 320      |\n","|    fps              | 603      |\n","|    time_elapsed     | 98       |\n","|    total_timesteps  | 59335    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.026    |\n","|    n_updates        | 29168    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 168      |\n","|    ep_rew_mean      | -168     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 330      |\n","|    fps              | 602      |\n","|    time_elapsed     | 101      |\n","|    total_timesteps  | 61030    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0302   |\n","|    n_updates        | 30016    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 169      |\n","|    ep_rew_mean      | -169     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 340      |\n","|    fps              | 601      |\n","|    time_elapsed     | 104      |\n","|    total_timesteps  | 62756    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0509   |\n","|    n_updates        | 30880    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 168      |\n","|    ep_rew_mean      | -168     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 350      |\n","|    fps              | 601      |\n","|    time_elapsed     | 106      |\n","|    total_timesteps  | 64318    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0531   |\n","|    n_updates        | 31656    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 167      |\n","|    ep_rew_mean      | -167     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 360      |\n","|    fps              | 601      |\n","|    time_elapsed     | 109      |\n","|    total_timesteps  | 65829    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0588   |\n","|    n_updates        | 32416    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 164      |\n","|    ep_rew_mean      | -164     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 370      |\n","|    fps              | 600      |\n","|    time_elapsed     | 112      |\n","|    total_timesteps  | 67293    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0487   |\n","|    n_updates        | 33144    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 161      |\n","|    ep_rew_mean      | -161     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 380      |\n","|    fps              | 600      |\n","|    time_elapsed     | 114      |\n","|    total_timesteps  | 68783    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0413   |\n","|    n_updates        | 33888    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 159      |\n","|    ep_rew_mean      | -159     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 390      |\n","|    fps              | 599      |\n","|    time_elapsed     | 117      |\n","|    total_timesteps  | 70197    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0397   |\n","|    n_updates        | 34600    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 156      |\n","|    ep_rew_mean      | -156     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 400      |\n","|    fps              | 598      |\n","|    time_elapsed     | 119      |\n","|    total_timesteps  | 71574    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0673   |\n","|    n_updates        | 35288    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 153      |\n","|    ep_rew_mean      | -153     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 410      |\n","|    fps              | 598      |\n","|    time_elapsed     | 121      |\n","|    total_timesteps  | 73001    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.102    |\n","|    n_updates        | 36000    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 152      |\n","|    ep_rew_mean      | -152     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 420      |\n","|    fps              | 597      |\n","|    time_elapsed     | 124      |\n","|    total_timesteps  | 74550    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.149    |\n","|    n_updates        | 36776    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 150      |\n","|    ep_rew_mean      | -150     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 430      |\n","|    fps              | 597      |\n","|    time_elapsed     | 127      |\n","|    total_timesteps  | 76041    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.135    |\n","|    n_updates        | 37520    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 149      |\n","|    ep_rew_mean      | -149     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 440      |\n","|    fps              | 596      |\n","|    time_elapsed     | 130      |\n","|    total_timesteps  | 77667    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.256    |\n","|    n_updates        | 38336    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 147      |\n","|    ep_rew_mean      | -147     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 450      |\n","|    fps              | 596      |\n","|    time_elapsed     | 132      |\n","|    total_timesteps  | 79039    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.171    |\n","|    n_updates        | 39016    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 145      |\n","|    ep_rew_mean      | -145     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 460      |\n","|    fps              | 596      |\n","|    time_elapsed     | 134      |\n","|    total_timesteps  | 80361    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.133    |\n","|    n_updates        | 39680    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 145      |\n","|    ep_rew_mean      | -145     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 470      |\n","|    fps              | 595      |\n","|    time_elapsed     | 137      |\n","|    total_timesteps  | 81813    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.233    |\n","|    n_updates        | 40408    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 144      |\n","|    ep_rew_mean      | -144     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 480      |\n","|    fps              | 595      |\n","|    time_elapsed     | 139      |\n","|    total_timesteps  | 83146    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.123    |\n","|    n_updates        | 41072    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 142      |\n","|    ep_rew_mean      | -142     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 490      |\n","|    fps              | 595      |\n","|    time_elapsed     | 141      |\n","|    total_timesteps  | 84399    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.175    |\n","|    n_updates        | 41696    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 141      |\n","|    ep_rew_mean      | -141     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 500      |\n","|    fps              | 594      |\n","|    time_elapsed     | 143      |\n","|    total_timesteps  | 85652    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.189    |\n","|    n_updates        | 42328    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 142      |\n","|    ep_rew_mean      | -142     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 510      |\n","|    fps              | 594      |\n","|    time_elapsed     | 146      |\n","|    total_timesteps  | 87197    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.201    |\n","|    n_updates        | 43096    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 139      |\n","|    ep_rew_mean      | -139     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 520      |\n","|    fps              | 594      |\n","|    time_elapsed     | 148      |\n","|    total_timesteps  | 88487    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.231    |\n","|    n_updates        | 43744    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 137      |\n","|    ep_rew_mean      | -137     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 530      |\n","|    fps              | 594      |\n","|    time_elapsed     | 151      |\n","|    total_timesteps  | 89761    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.201    |\n","|    n_updates        | 44384    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 133      |\n","|    ep_rew_mean      | -133     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 540      |\n","|    fps              | 594      |\n","|    time_elapsed     | 153      |\n","|    total_timesteps  | 90949    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.211    |\n","|    n_updates        | 44976    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 131      |\n","|    ep_rew_mean      | -131     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 550      |\n","|    fps              | 594      |\n","|    time_elapsed     | 155      |\n","|    total_timesteps  | 92092    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.14     |\n","|    n_updates        | 45544    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 128      |\n","|    ep_rew_mean      | -128     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 560      |\n","|    fps              | 593      |\n","|    time_elapsed     | 156      |\n","|    total_timesteps  | 93181    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.17     |\n","|    n_updates        | 46088    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 125      |\n","|    ep_rew_mean      | -125     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 570      |\n","|    fps              | 593      |\n","|    time_elapsed     | 159      |\n","|    total_timesteps  | 94362    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.12     |\n","|    n_updates        | 46680    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 123      |\n","|    ep_rew_mean      | -123     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 580      |\n","|    fps              | 593      |\n","|    time_elapsed     | 160      |\n","|    total_timesteps  | 95464    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.141    |\n","|    n_updates        | 47232    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 122      |\n","|    ep_rew_mean      | -122     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 590      |\n","|    fps              | 592      |\n","|    time_elapsed     | 162      |\n","|    total_timesteps  | 96555    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.0989   |\n","|    n_updates        | 47776    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 123      |\n","|    ep_rew_mean      | -123     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 600      |\n","|    fps              | 592      |\n","|    time_elapsed     | 165      |\n","|    total_timesteps  | 97910    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.118    |\n","|    n_updates        | 48456    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 118      |\n","|    ep_rew_mean      | -118     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 610      |\n","|    fps              | 592      |\n","|    time_elapsed     | 167      |\n","|    total_timesteps  | 98963    |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.14     |\n","|    n_updates        | 48984    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 118      |\n","|    ep_rew_mean      | -118     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 620      |\n","|    fps              | 592      |\n","|    time_elapsed     | 169      |\n","|    total_timesteps  | 100266   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.14     |\n","|    n_updates        | 49632    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 119      |\n","|    ep_rew_mean      | -119     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 630      |\n","|    fps              | 592      |\n","|    time_elapsed     | 171      |\n","|    total_timesteps  | 101613   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.118    |\n","|    n_updates        | 50304    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 120      |\n","|    ep_rew_mean      | -120     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 640      |\n","|    fps              | 591      |\n","|    time_elapsed     | 173      |\n","|    total_timesteps  | 102943   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.134    |\n","|    n_updates        | 50968    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 121      |\n","|    ep_rew_mean      | -121     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 650      |\n","|    fps              | 591      |\n","|    time_elapsed     | 176      |\n","|    total_timesteps  | 104209   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.164    |\n","|    n_updates        | 51608    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 122      |\n","|    ep_rew_mean      | -122     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 660      |\n","|    fps              | 591      |\n","|    time_elapsed     | 178      |\n","|    total_timesteps  | 105416   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.191    |\n","|    n_updates        | 52208    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 124      |\n","|    ep_rew_mean      | -124     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 670      |\n","|    fps              | 591      |\n","|    time_elapsed     | 180      |\n","|    total_timesteps  | 106768   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.213    |\n","|    n_updates        | 52880    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 127      |\n","|    ep_rew_mean      | -127     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 680      |\n","|    fps              | 591      |\n","|    time_elapsed     | 182      |\n","|    total_timesteps  | 108142   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.246    |\n","|    n_updates        | 53568    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 128      |\n","|    ep_rew_mean      | -128     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 690      |\n","|    fps              | 591      |\n","|    time_elapsed     | 185      |\n","|    total_timesteps  | 109388   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.196    |\n","|    n_updates        | 54192    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 129      |\n","|    ep_rew_mean      | -129     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 700      |\n","|    fps              | 590      |\n","|    time_elapsed     | 187      |\n","|    total_timesteps  | 110779   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.245    |\n","|    n_updates        | 54888    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 132      |\n","|    ep_rew_mean      | -132     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 710      |\n","|    fps              | 590      |\n","|    time_elapsed     | 189      |\n","|    total_timesteps  | 112180   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.241    |\n","|    n_updates        | 55592    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 132      |\n","|    ep_rew_mean      | -132     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 720      |\n","|    fps              | 590      |\n","|    time_elapsed     | 192      |\n","|    total_timesteps  | 113470   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.182    |\n","|    n_updates        | 56232    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 132      |\n","|    ep_rew_mean      | -132     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 730      |\n","|    fps              | 590      |\n","|    time_elapsed     | 194      |\n","|    total_timesteps  | 114802   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.17     |\n","|    n_updates        | 56904    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 131      |\n","|    ep_rew_mean      | -131     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 740      |\n","|    fps              | 590      |\n","|    time_elapsed     | 196      |\n","|    total_timesteps  | 116001   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.23     |\n","|    n_updates        | 57504    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 130      |\n","|    ep_rew_mean      | -130     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 750      |\n","|    fps              | 589      |\n","|    time_elapsed     | 198      |\n","|    total_timesteps  | 117180   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.246    |\n","|    n_updates        | 58088    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 129      |\n","|    ep_rew_mean      | -129     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 760      |\n","|    fps              | 589      |\n","|    time_elapsed     | 200      |\n","|    total_timesteps  | 118361   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.163    |\n","|    n_updates        | 58680    |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 128      |\n","|    ep_rew_mean      | -128     |\n","|    exploration_rate | 0.07     |\n","| time/               |          |\n","|    episodes         | 770      |\n","|    fps              | 589      |\n","|    time_elapsed     | 202      |\n","|    total_timesteps  | 119555   |\n","| train/              |          |\n","|    learning_rate    | 0.004    |\n","|    loss             | 0.235    |\n","|    n_updates        | 59280    |\n","----------------------------------\n","CPU times: user 3min 20s, sys: 2.79 s, total: 3min 23s\n","Wall time: 3min 23s\n"]},{"output_type":"execute_result","data":{"text/plain":["<stable_baselines3.dqn.dqn.DQN at 0x7f1570fc3f50>"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"tI8xh463Zi7M"},"source":["Evaluate after training, the mean episodic reward should have improved."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z952YogYZ8yD","outputId":"74572b99-bfc7-48a6-caea-8e4d5821bbea","executionInfo":{"status":"ok","timestamp":1644808047277,"user_tz":300,"elapsed":2589,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["mean_reward, std_reward = evaluate_policy(dqn_model, dqn_model.get_env(), deterministic=True, n_eval_episodes=20)\n","\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["mean_reward:-200.00 +/- 0.00\n"]}]},{"cell_type":"markdown","metadata":{"id":"xVm9QPNVwKXN"},"source":["### Prepare video recording"]},{"cell_type":"code","metadata":{"id":"MPyfQxD5z26J","executionInfo":{"status":"ok","timestamp":1644810536080,"user_tz":300,"elapsed":303,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["# Set up fake display; otherwise rendering will fail\n","import os\n","os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n","os.environ['DISPLAY'] = ':1'"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLzXxO8VMD6N","executionInfo":{"status":"ok","timestamp":1644810536608,"user_tz":300,"elapsed":3,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["import base64\n","from pathlib import Path\n","\n","from IPython import display as ipythondisplay\n","\n","def show_videos(video_path='', prefix=''):\n","  \"\"\"\n","  Taken from https://github.com/eleurent/highway-env\n","\n","  :param video_path: (str) Path to the folder containing videos\n","  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n","  \"\"\"\n","  html = []\n","  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n","      video_b64 = base64.b64encode(mp4.read_bytes())\n","      html.append('''<video alt=\"{}\" autoplay \n","                    loop controls style=\"height: 400px;\">\n","                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n","                </video>'''.format(mp4, video_b64.decode('ascii')))\n","  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"],"execution_count":62,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LTRNUfulOGaF"},"source":["We will record a video using the [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."]},{"cell_type":"code","metadata":{"id":"Trag9dQpOIhx","executionInfo":{"status":"ok","timestamp":1644810536609,"user_tz":300,"elapsed":3,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n","\n","def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n","  \"\"\"\n","  :param env_id: (str)\n","  :param model: (RL model)\n","  :param video_length: (int)\n","  :param prefix: (str)\n","  :param video_folder: (str)\n","  \"\"\"\n","  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n","  # Start the video at step=0 and record 500 steps\n","  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n","                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n","                              name_prefix=prefix)\n","\n","  obs = eval_env.reset()\n","  for _ in range(video_length):\n","    action, _ = model.predict(obs, deterministic=False)\n","    obs, _, _, _ = eval_env.step(action)\n","\n","  # Close the video recorder\n","  eval_env.close()"],"execution_count":63,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFiOqKE3aDzI"},"source":["## Visualize trained agent"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"MvVGX13xaGbf","outputId":"4138f91a-9979-431a-b009-c7bde4c92e36","executionInfo":{"status":"error","timestamp":1644810582782,"user_tz":300,"elapsed":431,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["record_video('MountainCar-v0', dqn_model, video_length=500, prefix='dqn-mountaincar')"],"execution_count":67,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-ae3c9367efcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecord_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MountainCar-v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdqn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dqn-mountaincar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-63-82a985f10056>\u001b[0m in \u001b[0;36mrecord_video\u001b[0;34m(env_id, model, video_length, prefix, video_folder)\u001b[0m\n\u001b[1;32m     15\u001b[0m                               name_prefix=prefix)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvObs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\u001b[0m in \u001b[0;36mstart_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorded_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecording\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ansi\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \"\"\"\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     raise ImportError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"]}]},{"cell_type":"code","metadata":{"id":"tHaYvk8uaK70","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1644810607807,"user_tz":300,"elapsed":291,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}},"outputId":"9f82af8b-c220-4501-8d10-ce961c8ebe78"},"source":["show_videos('videos', prefix='dqn')"],"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"TY00dtf4aRLQ"},"source":["## Visualize Q-values"]},{"cell_type":"markdown","metadata":{"id":"cP3YH2x7rieM"},"source":["### Exercise (5 minutes): Retrieve q-values\n","\n","The function will be used to retrieve the learned q-values for a given state (`observation` in the code).\n","\n","The q-network from SB3 DQN can be accessed via `model.q_net` and is a PyTorch module (you can therefore call `.forward()` on it).\n","\n","You need to convert the observation to a PyTorch tensor and then convert the resulting q-values to numpy array.\n","\n","Note: It is recommended to use `with th.no_grad():` context to save computation and memory"]},{"cell_type":"code","metadata":{"id":"QiCp8OpCbKZW","executionInfo":{"status":"ok","timestamp":1644809645797,"user_tz":300,"elapsed":1,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":[" \n","def get_q_values(model: DQN, obs: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Retrieve Q-values for a given observation.\n","\n","    :param model: a DQN model\n","    :param obs: a single observation\n","    :return: the associated q-values for the given observation\n","    \"\"\"\n","    assert model.get_env().observation_space.contains(obs), f\"Invalid observation of shape {obs.shape}: {obs}\"\n","    ### YOUR CODE HERE\n","    # Retrieve q-values for the given observation and convert them to numpy\n","    with th.no_grad():\n","      tensor_obs = th.from_numpy(obs).to(model.device).float().unsqueeze(0)\n","      q_values = model.q_net(tensor_obs).cpu().detach().numpy()[0]\n","\n","    ### END OF YOUR CODE\n","    assert isinstance(q_values, np.ndarray), \"The returned q_values is not a numpy array\"\n","    assert q_values.shape == (3,), f\"Wrong shape: (3,) was expected but got {q_values.shape}\"\n","\n","    return q_values"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcudYVkdbB0e"},"source":["### Q-values for the initial state\n","\n","Let's reset the environment to start a new episode:"]},{"cell_type":"code","metadata":{"id":"izFixcWgaVe3","executionInfo":{"status":"ok","timestamp":1644810793250,"user_tz":300,"elapsed":301,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["obs = env.reset()"],"execution_count":82,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ouWnFaut0KB"},"source":["we plot the rendered environment to visualize it"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":560},"id":"NF1L_1Gfal-g","outputId":"78411596-4161-4500-acbc-1f0f9d9c038d","executionInfo":{"status":"error","timestamp":1644810615834,"user_tz":300,"elapsed":307,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["plt.axis('off')\n","plt.imshow(env.render(mode=\"rgb_array\"))"],"execution_count":70,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-ae01f39708ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     raise ImportError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADKUlEQVR4nO3UMQEAIAzAMMC/5+GiHCQKenXPzAKgcV4HAPzEdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIHQBcjcEy3+fc28AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"rtKIGyJ-bEQO"},"source":["### Exercise (5 minutes): predict taken action according to q-values\n","\n","Using the `get_q_values()` function, retrieve the q-values for the initial observation, print them for each action (\"left\", \"nothing\", \"right\") and print the action that the greedy (deterministic) policy would follow (i.e., the action with the highest q-value for that state)."]},{"cell_type":"code","metadata":{"id":"BPegCYbSuY-f","executionInfo":{"status":"ok","timestamp":1644808859468,"user_tz":300,"elapsed":374,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["action_str = [\"Left\", \"Nothing\", \"Right\"]  # action=0 -> go left, action=1 -> do nothing, action=2 -> go right"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"bv90MTHvaqbV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644810798629,"user_tz":300,"elapsed":301,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}},"outputId":"c43b434f-f15b-418b-928a-a8611884a02a"},"source":["### YOUR CODE HERE\n","# Retrieve q-values for the initial state\n","# You should use `get_q_values()`\n","q_values = get_q_values(dqn_model, obs)\n","q_value_left, q_value_nothing, q_value_right = q_values\n","### END OF YOUR CODE\n","\n","print(f\"Q-value of the initial state left={q_value_left:.2f} nothing={q_value_nothing:.2f} right={q_value_right:.2f}\")\n","\n","### YOUR CODE HERE\n","# Compute the action taken in the initilal state according to q-values \n","# when following a greedy strategy\n","action = np.argmax(q_values)\n","\n","## END of your code here\n","\n","print(f\"Action taken by the greedy policy in the initial state: {action_str[action]}\")"],"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-value of the initial state left=-64.06 nothing=-63.56 right=-63.13\n","Action taken by the greedy policy in the initial state: Right\n"]}]},{"cell_type":"markdown","metadata":{"id":"ov0UwoBzcaDv"},"source":["The q-value of the initial state corresponds to how much (discounted) reward the agent expects to get in this episode.\n","\n","We will compare the estimated q-value to the discounted return of the episode."]},{"cell_type":"code","metadata":{"id":"dhhF-GJccVne","executionInfo":{"status":"ok","timestamp":1644810801244,"user_tz":300,"elapsed":279,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}}},"source":["initial_q_value = q_values.max()"],"execution_count":84,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JueeSE1xcQpK"},"source":["## Step until the end of the episode\n","\n"]},{"cell_type":"code","metadata":{"id":"V_OYobAab8SF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644810804255,"user_tz":300,"elapsed":294,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}},"outputId":"7a5ffde3-3254-4e65-fbec-fead6ae089c3"},"source":["episode_rewards = []\n","done = False\n","i = 0\n","\n","while not done:\n","    i += 1\n","\n","    # Display current state\n","    plt.imshow(env.render(mode=\"rgb_array\"))\n","    plt.show()\n","\n","    # Retrieve q-value\n","    q_values = get_q_values(dqn_model, obs)\n","\n","    # Take greedy-action\n","    action, _ = dqn_model.predict(obs, deterministic=True)\n","\n","    print(f\"Q-value of the current state left={q_values[0]:.2f} nothing={q_values[1]:.2f} right={q_values[2]:.2f}\")\n","    print(f\"Action: {action_str[action]}\")\n","\n","    obs, reward, done, info = env.step(action)\n","\n","    episode_rewards.append(reward)\n"],"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-value of the current state left=-64.06 nothing=-63.56 right=-63.13\n","Action: Right\n","Q-value of the current state left=-63.97 nothing=-63.39 right=-62.93\n","Action: Right\n","Q-value of the current state left=-63.74 nothing=-63.10 right=-62.65\n","Action: Right\n","Q-value of the current state left=-63.48 nothing=-62.89 right=-62.48\n","Action: Right\n","Q-value of the current state left=-63.19 nothing=-62.66 right=-62.29\n","Action: Right\n","Q-value of the current state left=-62.87 nothing=-62.42 right=-62.10\n","Action: Right\n","Q-value of the current state left=-62.53 nothing=-62.17 right=-61.90\n","Action: Right\n","Q-value of the current state left=-62.22 nothing=-61.93 right=-61.72\n","Action: Right\n","Q-value of the current state left=-61.94 nothing=-61.72 right=-61.57\n","Action: Right\n","Q-value of the current state left=-61.71 nothing=-61.52 right=-61.35\n","Action: Right\n","Q-value of the current state left=-61.51 nothing=-61.33 right=-61.15\n","Action: Right\n","Q-value of the current state left=-61.28 nothing=-61.14 right=-60.94\n","Action: Right\n","Q-value of the current state left=-61.05 nothing=-60.93 right=-60.72\n","Action: Right\n","Q-value of the current state left=-60.82 nothing=-60.74 right=-60.53\n","Action: Right\n","Q-value of the current state left=-60.58 nothing=-60.54 right=-60.32\n","Action: Right\n","Q-value of the current state left=-60.32 nothing=-60.31 right=-60.09\n","Action: Right\n","Q-value of the current state left=-60.06 nothing=-60.09 right=-59.86\n","Action: Right\n","Q-value of the current state left=-59.80 nothing=-59.87 right=-59.64\n","Action: Right\n","Q-value of the current state left=-59.53 nothing=-59.65 right=-59.42\n","Action: Right\n","Q-value of the current state left=-59.26 nothing=-59.43 right=-59.22\n","Action: Right\n","Q-value of the current state left=-59.00 nothing=-59.22 right=-59.02\n","Action: Left\n","Q-value of the current state left=-58.24 nothing=-58.52 right=-58.26\n","Action: Left\n","Q-value of the current state left=-57.25 nothing=-57.56 right=-57.37\n","Action: Left\n","Q-value of the current state left=-56.41 nothing=-56.84 right=-56.97\n","Action: Left\n","Q-value of the current state left=-56.17 nothing=-56.72 right=-57.13\n","Action: Left\n","Q-value of the current state left=-56.27 nothing=-56.83 right=-57.25\n","Action: Left\n","Q-value of the current state left=-56.00 nothing=-56.55 right=-56.98\n","Action: Left\n","Q-value of the current state left=-55.51 nothing=-56.01 right=-56.38\n","Action: Left\n","Q-value of the current state left=-54.99 nothing=-55.36 right=-55.51\n","Action: Left\n","Q-value of the current state left=-54.50 nothing=-54.77 right=-54.85\n","Action: Left\n","Q-value of the current state left=-54.08 nothing=-54.28 right=-54.36\n","Action: Left\n","Q-value of the current state left=-53.70 nothing=-53.83 right=-53.91\n","Action: Left\n","Q-value of the current state left=-53.27 nothing=-53.34 right=-53.43\n","Action: Left\n","Q-value of the current state left=-52.55 nothing=-52.55 right=-52.64\n","Action: Nothing\n","Q-value of the current state left=-52.39 nothing=-52.38 right=-52.47\n","Action: Nothing\n","Q-value of the current state left=-52.28 nothing=-52.40 right=-52.51\n","Action: Left\n","Q-value of the current state left=-51.81 nothing=-52.03 right=-52.15\n","Action: Left\n","Q-value of the current state left=-51.76 nothing=-52.14 right=-52.28\n","Action: Left\n","Q-value of the current state left=-51.78 nothing=-52.17 right=-52.32\n","Action: Left\n","Q-value of the current state left=-51.77 nothing=-52.04 right=-52.19\n","Action: Left\n","Q-value of the current state left=-51.40 nothing=-51.48 right=-51.59\n","Action: Left\n","Q-value of the current state left=-51.21 nothing=-51.28 right=-51.38\n","Action: Left\n","Q-value of the current state left=-51.09 nothing=-51.14 right=-51.23\n","Action: Left\n","Q-value of the current state left=-51.00 nothing=-51.21 right=-51.44\n","Action: Left\n","Q-value of the current state left=-50.74 nothing=-51.05 right=-51.34\n","Action: Left\n","Q-value of the current state left=-50.06 nothing=-50.37 right=-50.64\n","Action: Left\n","Q-value of the current state left=-50.38 nothing=-50.69 right=-50.99\n","Action: Left\n","Q-value of the current state left=-51.33 nothing=-51.66 right=-52.00\n","Action: Left\n","Q-value of the current state left=-51.12 nothing=-51.44 right=-51.80\n","Action: Left\n","Q-value of the current state left=-50.19 nothing=-50.23 right=-50.41\n","Action: Left\n","Q-value of the current state left=-49.23 nothing=-48.98 right=-48.95\n","Action: Right\n","Q-value of the current state left=-48.95 nothing=-48.61 right=-48.56\n","Action: Right\n","Q-value of the current state left=-48.61 nothing=-48.23 right=-48.16\n","Action: Right\n","Q-value of the current state left=-47.72 nothing=-47.24 right=-47.16\n","Action: Right\n","Q-value of the current state left=-46.95 nothing=-46.28 right=-46.55\n","Action: Nothing\n","Q-value of the current state left=-45.73 nothing=-44.88 right=-45.09\n","Action: Nothing\n","Q-value of the current state left=-44.70 nothing=-43.77 right=-43.91\n","Action: Nothing\n","Q-value of the current state left=-44.50 nothing=-43.41 right=-43.42\n","Action: Nothing\n","Q-value of the current state left=-45.17 nothing=-43.85 right=-43.59\n","Action: Right\n","Q-value of the current state left=-45.35 nothing=-43.69 right=-42.95\n","Action: Right\n","Q-value of the current state left=-45.63 nothing=-43.62 right=-42.37\n","Action: Right\n","Q-value of the current state left=-44.01 nothing=-41.96 right=-40.55\n","Action: Right\n","Q-value of the current state left=-43.62 nothing=-41.40 right=-39.66\n","Action: Right\n","Q-value of the current state left=-42.87 nothing=-40.46 right=-38.28\n","Action: Right\n","Q-value of the current state left=-42.03 nothing=-39.51 right=-37.00\n","Action: Right\n","Q-value of the current state left=-42.40 nothing=-39.59 right=-36.34\n","Action: Right\n","Q-value of the current state left=-43.79 nothing=-40.66 right=-36.66\n","Action: Right\n","Q-value of the current state left=-43.31 nothing=-40.12 right=-35.73\n","Action: Right\n","Q-value of the current state left=-42.55 nothing=-39.26 right=-34.45\n","Action: Right\n","Q-value of the current state left=-41.68 nothing=-38.29 right=-33.08\n","Action: Right\n","Q-value of the current state left=-40.99 nothing=-37.44 right=-32.11\n","Action: Right\n","Q-value of the current state left=-38.89 nothing=-35.52 right=-30.74\n","Action: Right\n","Q-value of the current state left=-38.14 nothing=-34.52 right=-29.93\n","Action: Right\n","Q-value of the current state left=-38.27 nothing=-34.73 right=-30.03\n","Action: Right\n","Q-value of the current state left=-38.90 nothing=-35.23 right=-29.79\n","Action: Right\n","Q-value of the current state left=-38.03 nothing=-34.49 right=-28.81\n","Action: Right\n","Q-value of the current state left=-36.06 nothing=-32.93 right=-27.70\n","Action: Right\n","Q-value of the current state left=-35.18 nothing=-32.29 right=-27.26\n","Action: Right\n","Q-value of the current state left=-35.65 nothing=-32.95 right=-27.82\n","Action: Right\n","Q-value of the current state left=-33.31 nothing=-30.76 right=-25.43\n","Action: Right\n","Q-value of the current state left=-32.48 nothing=-29.95 right=-24.71\n","Action: Right\n","Q-value of the current state left=-32.04 nothing=-29.53 right=-24.31\n","Action: Right\n","Q-value of the current state left=-31.75 nothing=-29.20 right=-23.87\n","Action: Right\n","Q-value of the current state left=-31.53 nothing=-28.82 right=-23.14\n","Action: Right\n","Q-value of the current state left=-31.71 nothing=-28.86 right=-22.84\n","Action: Right\n","Q-value of the current state left=-29.66 nothing=-27.05 right=-21.53\n","Action: Right\n","Q-value of the current state left=-26.85 nothing=-24.60 right=-19.84\n","Action: Right\n","Q-value of the current state left=-25.00 nothing=-22.93 right=-18.61\n","Action: Right\n","Q-value of the current state left=-23.70 nothing=-21.71 right=-17.65\n","Action: Right\n","Q-value of the current state left=-22.55 nothing=-20.64 right=-16.79\n","Action: Right\n","Q-value of the current state left=-21.00 nothing=-19.25 right=-15.82\n","Action: Right\n","Q-value of the current state left=-19.41 nothing=-17.83 right=-14.86\n","Action: Right\n","Q-value of the current state left=-17.86 nothing=-16.45 right=-13.92\n","Action: Right\n","Q-value of the current state left=-16.30 nothing=-15.06 right=-12.98\n","Action: Right\n","Q-value of the current state left=-14.70 nothing=-13.64 right=-12.02\n","Action: Right\n","Q-value of the current state left=-13.03 nothing=-12.15 right=-11.02\n","Action: Right\n","Q-value of the current state left=-11.96 nothing=-11.19 right=-10.35\n","Action: Right\n","Q-value of the current state left=-11.29 nothing=-10.50 right=-9.64\n","Action: Right\n","Q-value of the current state left=-10.54 nothing=-9.72 right=-8.83\n","Action: Right\n","Q-value of the current state left=-9.74 nothing=-8.89 right=-7.99\n","Action: Right\n","Q-value of the current state left=-8.70 nothing=-7.81 right=-6.87\n","Action: Right\n","Q-value of the current state left=-7.44 nothing=-6.50 right=-5.52\n","Action: Right\n","Q-value of the current state left=-6.22 nothing=-5.23 right=-4.21\n","Action: Right\n","Q-value of the current state left=-5.11 nothing=-4.07 right=-3.02\n","Action: Right\n","Q-value of the current state left=-4.00 nothing=-2.91 right=-1.82\n","Action: Right\n","Q-value of the current state left=-2.81 nothing=-1.68 right=-0.54\n","Action: Right\n","Q-value of the current state left=-1.51 nothing=-0.32 right=0.86\n","Action: Right\n","Q-value of the current state left=-0.09 nothing=1.17 right=2.40\n","Action: Right\n","Q-value of the current state left=1.47 nothing=2.79 right=4.08\n","Action: Right\n"]}]},{"cell_type":"markdown","metadata":{"id":"YBEomET1wkjN"},"source":["### Exercise (3 minutes): compare estimated initial q-value with actual discounted return\n","\n","Compute the discounted return (sum of discounted reward) of the episode and compare it to the initial estimated q-value.\n","\n","Note: You will need to use the discount factor `dqn_model.gamma`"]},{"cell_type":"code","metadata":{"id":"Om4NNW2VdnM9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644811546454,"user_tz":300,"elapsed":348,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}},"outputId":"85294a37-3f47-414c-97be-01fc2d022c1b"},"source":["### YOUR CODE HERE\n","# Compute the sum of discounted reward for the last episode\n","# using `episode_rewards` list and `dqn_model.gamma` discount factor\n","\n","discount = np.array([dqn_model.gamma ** i for i in range(len(episode_rewards))])\n","sum_discounted_rewards = np.dot(np.array(episode_rewards), discount).sum()\n","\n","### END OF YOUR CODE\n","\n","print(f\"Sum discounted rewards: {sum_discounted_rewards:.2f}, initial q-value {initial_q_value:.2f}\")"],"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["Sum discounted rewards: -66.56, initial q-value -63.13\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZloxyPKPg9HX"},"source":["## Exercise (30 minutes): Double DQN\n","\n","In DQN, the same network is responsible for selecting and estimating the best next action (in the TD-target) and that may lead to over-estimation (the action which q-value is over-estimated will be chosen more often and this slow down training).\n","\n","To reduce over-estimation, double q-learning (and then double DQN) was proposed. It decouples the action selection from the value estimation.\n","\n","Concretely, in DQN, the target q-value is defined as:\n","\n","$$Y^{DQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right); \\mathbb{\\theta}_{target}\\right)$$\n","\n","where the target network `q_net_target` with parameters $\\mathbb{\\theta}_{target}$ is used for both action selection and estimation, and can therefore be rewritten:\n","\n","$$Y^{DQN}_{t} = r_{t+1} + \\gamma \\max_{a}{Q}\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right)$$\n","\n","Double DQN uses the online network `q_net` with parameters $\\mathbb{\\theta}_{online}$ to select the action and the target network `q_net_target` to estimate the associated q-values:\n","\n","$$Y^{DoubleDQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{online}\\right); \\mathbb{\\theta}_{target}\\right)$$\n","\n","\n","The goal in this exercise is for you to write the update method for `DoubleDQN`.\n","\n","You will need to:\n","\n","1. Sample replay buffer data using `self.replay_buffer.sample(batch_size)`\n","\n","2. Compute the Double DQN target q-value using the next observations `replay_data.next_observation`, the online network `self.q_net`, the target network `self.q_net_target`, the rewards `replay_data.rewards` and the termination signals `replay_data.dones`. Be careful with the shape of each object ;)\n","\n","3. Compute the current q-value estimates using the online network `self.q_net`, the current observations `replay_data.observations` and the buffer actions `replay_data.actions`\n","\n","4. Compute the loss to train the q-network using L2 or Huber loss (`F.smooth_l1_loss`)\n","\n","\n","Link: https://paperswithcode.com/method/double-q-learning\n","\n","Paper: https://arxiv.org/abs/1509.06461\n","\n"]},{"cell_type":"code","metadata":{"id":"4227ILqjg8b4"},"source":["from torch.nn import functional as F\n","\n","class DoubleDQN(DQN):\n","    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n","        # Switch to train mode (this affects batch norm / dropout)\n","        self.policy.set_training_mode(True)\n","        # Update learning rate according to schedule\n","        self._update_learning_rate(self.policy.optimizer)\n","\n","        losses = []\n","        for _ in range(gradient_steps):\n","            ### YOUR CODE HERE\n","            # Sample replay buffer\n","            replay_data = ...\n","\n","            # Do not backpropagate gradient to the target network\n","            with th.no_grad():\n","                # Compute the next Q-values using the target network\n","                next_q_values = ...\n","                # Decouple action selection from value estimation\n","                # Compute q-values for the next observation using the online q net\n","                next_q_values_online = ...\n","                # Select action with online network\n","                next_actions_online = ...\n","                # Estimate the q-values for the selected actions using target q network\n","                next_q_values = ...\n","               \n","                # 1-step TD target\n","                target_q_values = ...\n","\n","            # Get current Q-values estimates\n","            current_q_values = ...\n","\n","            # Retrieve the q-values for the actions from the replay buffer\n","            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n","\n","            # Check the shape\n","            assert current_q_values.shape == target_q_values.shape\n","\n","            # Compute loss (L2 or Huber loss)\n","            loss = ...\n","\n","            ### END OF YOUR CODE\n","            \n","            losses.append(loss.item())\n","\n","            # Optimize the q-network\n","            self.policy.optimizer.zero_grad()\n","            loss.backward()\n","            # Clip gradient norm\n","            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n","            self.policy.optimizer.step()\n","\n","        # Increase update counter\n","        self._n_updates += gradient_steps\n","\n","        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n","        self.logger.record(\"train/loss\", np.mean(losses))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dG3fpWWOg_AS"},"source":["## Monitoring Evolution of the Estimated q-value\n","\n","\n","Here we create a SB3 callback to over-estimate initial q-values and then monitor their evolution over time."]},{"cell_type":"code","metadata":{"id":"BLbQ9RhUpMOl"},"source":["from torch.nn import functional as F\n","\n","from stable_baselines3.common.callbacks import BaseCallback\n","\n","\n","class MonitorQValueCallback(BaseCallback):\n","    \"\"\"\n","    Callback to monitor the evolution of the q-value\n","    for the initial state.\n","    It allows to artificially over-estimate a q-value for initial states.\n","\n","    \"\"\"\n","    def __init__(self, sample_interval: int = 2500):\n","        super().__init__()\n","        self.timesteps = []\n","        self.max_q_values = []\n","        self.sample_interval = sample_interval\n","        n_samples = 512\n","        env = gym.make(\"MountainCar-v0\")\n","        # Sample initial states that will be used to monitor the estimated q-value\n","        self.start_obs = np.array([env.reset() for _ in range(n_samples)])\n","    \n","    def _on_training_start(self) -> None:\n","        # Create overestimation\n","        obs = th.tensor(self.start_obs, device=self.model.device).float()\n","        # Over-estimate going left q-value for the initial states\n","        target_q_values = th.ones((len(obs), 1), device=self.model.device).float() * 100\n","\n","        for _ in range(100):\n","            # Get current Q-values estimates\n","            current_q_values = self.model.q_net(obs)\n","\n","            # Over-estimate going left\n","            current_q_values = th.gather(current_q_values, dim=1, index=th.zeros((len(obs), 1), device=self.model.device).long())\n","\n","            loss = F.mse_loss(current_q_values, target_q_values)\n","\n","            # Optimize the policy\n","            self.model.policy.optimizer.zero_grad()\n","            loss.backward()\n","            self.model.policy.optimizer.step()\n","\n","    def _on_step(self) -> bool:\n","        # Sample q-values\n","        if self.n_calls % self.sample_interval == 0:\n","            # Monitor estimated q-values using current model\n","            obs = th.tensor(self.start_obs, device=self.model.device).float()\n","            with th.no_grad():\n","                q_values = self.model.q_net(obs).cpu().numpy()\n","\n","            self.logger.record(\"train/max_q_value\", float(q_values.max()))\n","            self.timesteps.append(self.num_timesteps)\n","            self.max_q_values.append(q_values.max())\n","        return True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36dtW1c4xQUG"},"source":["## Evolution of the q-value with initial over-estimation"]},{"cell_type":"markdown","metadata":{"id":"j-XTmT6SxdOa"},"source":["### DQN"]},{"cell_type":"code","metadata":{"id":"ZIo1EDx2xcwZ"},"source":["dqn_model = DQN(\"MlpPolicy\",\n","            \"MountainCar-v0\",\n","            verbose=1,\n","            train_freq=16,\n","            gradient_steps=8,\n","            gamma=0.99,\n","            exploration_fraction=0.2,\n","            exploration_final_eps=0.07,\n","            target_update_interval=5000,\n","            learning_starts=1000,\n","            buffer_size=25000,\n","            batch_size=128,\n","            learning_rate=4e-3,\n","            policy_kwargs=dict(net_arch=[256, 256]),\n","            tensorboard_log=tensorboard_log,\n","            seed=102)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vd62HoWsxfBJ"},"source":["Define the callback"]},{"cell_type":"code","metadata":{"id":"wcULdR48xhH5"},"source":["monitor_dqn_value_cb = MonitorQValueCallback()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Z_cFIapxhR7"},"source":["dqn_model.learn(total_timesteps=int(4e4), callback=monitor_dqn_value_cb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NxAGOezBx3GS"},"source":["### Double DQN"]},{"cell_type":"code","metadata":{"id":"xYNcgKsSegj0"},"source":["double_q = DoubleDQN(\"MlpPolicy\",\n","            \"MountainCar-v0\",\n","            verbose=1,\n","            train_freq=16,\n","            gradient_steps=8,\n","            gamma=0.99,\n","            exploration_fraction=0.2,\n","            exploration_final_eps=0.07,\n","            target_update_interval=5000,\n","            learning_starts=1000,\n","            buffer_size=25000,\n","            batch_size=128,\n","            learning_rate=4e-3,\n","            policy_kwargs=dict(net_arch=[256, 256]),\n","            tensorboard_log=tensorboard_log,\n","            seed=102)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JWhQTCXQyBJZ"},"source":["monitor_double_q_value_cb = MonitorQValueCallback()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mvveywQUkEpW"},"source":["double_q.learn(int(4e4), log_interval=10, callback=monitor_double_q_value_cb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rPqcCnbnyIR5"},"source":["### Evolution of the max q-value for start states over time"]},{"cell_type":"code","metadata":{"id":"jNeKzUoaa6_K"},"source":["plt.figure(figsize=(6, 3), dpi=200)\n","plt.title(\"Evolution of max q-value for start states over time\")\n","plt.plot(monitor_dqn_value_cb.timesteps, monitor_dqn_value_cb.max_q_values, label=\"DQN\")\n","plt.plot(monitor_double_q_value_cb.timesteps, monitor_double_q_value_cb.max_q_values, label=\"Double DQN\")\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqRixViAv6gT"},"source":[""],"execution_count":null,"outputs":[]}]}